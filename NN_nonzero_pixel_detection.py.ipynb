{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+qWjxLTkyQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1146d4ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot array as an image\n",
    "def imshow(npimg):\n",
    "    plt.imshow(npimg)\n",
    "    plt.show()\n",
    "# create an array(16X16) of zeros and set a random coordinate (i,j) to 1\n",
    "def create_fake_dataset(imgsz):\n",
    "    imgs = np.zeros((numimgs, imgsz, imgsz))\n",
    "    coordinates = np.zeros((numimgs, 2))\n",
    "    for i in range(numimgs):\n",
    "        rand_coord = np.ravel((np.random.randint(15, size=(1, 2))).tolist())\n",
    "        imgs[i, rand_coord[0], rand_coord[1]] = 1\n",
    "        coordinates[i] = rand_coord\n",
    "    imshow(imgs[19]) #display random image\n",
    "    return imgs, coordinates\n",
    "\n",
    "numimgs = 10000\n",
    "imgs, coordinates = create_fake_dataset(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape to vector\n",
    "X = (imgs.reshape(numimgs, -1) - np.mean(imgs)) / np.std(imgs)\n",
    "np.mean(X), np.std(X)\n",
    "y = coordinates.reshape(numimgs, -1)\n",
    "\n",
    "#split data to train, test\n",
    "i = int(0.8*numimgs)\n",
    "X_train = Variable(torch.from_numpy(X[:i]).float())\n",
    "y_train = Variable(torch.from_numpy(y[:i]).float())\n",
    "X_test = Variable(torch.from_numpy(X[i:]).float())\n",
    "y_test = Variable(torch.from_numpy(y[i:]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple NN with one hidden layer for localization of non-zero pixel\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,X):\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b would be sufficient\n",
    "        self.main = nn.Sequential(nn.Linear(X.shape[-1], 256), nn.ReLU(True),nn.Dropout(p=0.2),nn.Linear(256,y.shape[-1]))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "model = Net(X)\n",
    "# Loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.003, momentum= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(66.2970, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(60.5919, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(61.4224, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(52.3873, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(53.1481, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(42.6192, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(43.2556, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(33.1055, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(33.5869, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(25.3136, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(25.6419, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(20.1703, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(20.3553, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(17.9564, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(17.9450, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(18.2924, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(18.1704, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(20.2402, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(20.0604, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(22.5667, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(22.3785, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(24.1163, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(23.8479, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(24.1778, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(23.8375, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(22.6464, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(22.3318, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(19.9377, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(19.5685, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(16.7355, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(16.3581, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(13.6855, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(13.3188, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(11.2345, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(10.8874, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(9.5587, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(9.2579, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(8.6150, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(8.3527, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(8.2307, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(8.0404, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(8.1833, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(8.0152, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(8.2628, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(8.0935, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(8.3052, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(8.1471, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(8.2047, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(8.0950, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(7.9138, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(7.8089, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(7.4362, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(7.3276, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(6.8130, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(6.7327, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(6.1068, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(6.0350, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(5.3862, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(5.2946, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(4.7149, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(4.6869, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(4.1349, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(4.1128, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(3.6643, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(3.6781, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(3.2961, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(3.3682, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(3.0042, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(3.1260, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(2.7522, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(2.8948, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(2.5055, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(2.7173, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(2.2385, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(2.4082, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(1.9438, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(2.1464, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(1.6288, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(1.8451, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(1.3134, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(1.5495, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(1.0205, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(1.2723, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.7703, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(1.0314, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.5767, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.8586, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.4450, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.7396, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.3706, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.6820, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.3422, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.6568, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.3444, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.6788, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.3616, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.6876, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.3808, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.7320, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.3916, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.7577, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.3890, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.7605, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.3719, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.7402, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.3427, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.7077, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.3062, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.6690, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.2671, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.6336, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.2298, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.5928, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.1972, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.5625, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.1704, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.5191, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.1494, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.5164, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.1327, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.4836, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.1186, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.4734, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.1055, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.4663, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0922, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.4513, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0788, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.4205, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0658, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.4135, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0538, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3941, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0440, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3875, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0370, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3862, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0332, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3633, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0325, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3639, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0344, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3650, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0380, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3681, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0421, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3626, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0458, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3597, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0485, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3665, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0496, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3660, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0491, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3690, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0470, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3615, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0439, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3707, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0399, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3589, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0356, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3597, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0313, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3520, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0275, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.3471, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0241, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3417, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0213, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3485, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0190, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3382, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0170, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3474, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0153, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3495, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3367, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0121, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3428, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0107, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3276, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3342, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3384, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3358, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3322, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3359, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3344, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3326, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.3462, grad_fn=<MseLossBackward>)\n",
      "val_loss: tensor(0.0080, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "max_epochs = 100\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(X_train)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, y_train)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer.step()\n",
    "    print ('loss:',loss)\n",
    "\n",
    "    # Validating the model\n",
    "    model.eval()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(X_test)\n",
    "    # calculate the batch loss\n",
    "    val_loss = criterion(output, y_test)\n",
    "    print ('val_loss:',val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
